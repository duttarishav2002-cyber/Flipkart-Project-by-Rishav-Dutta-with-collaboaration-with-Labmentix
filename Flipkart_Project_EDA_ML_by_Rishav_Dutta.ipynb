{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duttarishav2002-cyber/Flipkart-Project-by-Rishav-Dutta-with-collaboaration-with-Labmentix/blob/main/Flipkart_Project_EDA_ML_by_Rishav_Dutta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project aims to develop a predictive machine learning model to estimate Customer Satisfaction (CSAT) scores based on historical customer support data. Accurate prediction of CSAT enables proactive intervention, improved customer retention, and data-driven enhancements to support operations.\n",
        "\n",
        "The workflow begins with comprehensive data preprocessing, including handling of missing values using median and mode imputation, removal of irrelevant columns, and conversion of date fields. A new feature-response time in minutes - was engineered by calculating the time difference between issue reporting and response timestamps.\n",
        "\n",
        "Categorical variables such as channel name, category, Sub-category, Tenure Bucket, and Agent Shift were transformed using one-hot encoding, and all numeric variables were standardized using StandardScaler where applicable. To address the severe class imbalance in CSAT scores, SMOTE (Synthetic Minority Oversampling Technique) was applied to oversample minority classes in the training set.\n",
        "\n",
        "Multiple classification models were trained and evaluated, including:\n",
        "\n",
        "Random Forest Classifier Logistic Regression XGBoost Classifier Each model was assessed asing accuracy, precision, recall, and F1-score. Among them, Random Forest showed the best overall performance and robustness, especially in handling the class imbalance. Hyperparameter tuning was considered for further optimization.\n",
        "\n",
        "Feature importance analysis using Random Forest revealed that channel name, agent Tenure Bucket, Agent Shift, and specific Sub-category types significantly influenced CSAT scores, offering meaningful business insights.\n",
        "\n",
        "This project not only delivers a reliable predictive model but also provides actionable recommendations for improving service quality - such as enhancing agent training, optimizing shift schedules, and prioritizing high-friction customer issues. It establishes a foundation for Integrating real-time CSAT prediction into customer support systems, enabling continuous performance monitoring and proactive decision-making."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here.\n",
        "\n"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flipkart's customer support team handles a large volume of customer interactions daily, covering various issues such as product queries, order-related concerns, returns, and cancellations. However, analyzing these interactions to understand the factors driving customer satisfaction remains a key challenge. This project aims to analyze customer support data to identify the major factors influencing Customer Satisfaction (CSAT) Scores and to develop predictive models that can classify CSAT scores accurately. The goal is to provide actionable insights that can help improve customer service strategies and enhance overall customer experience."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import io # Needed to read data when pasted as a string"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "import pandas as pd\n",
        "import io\n",
        "df = pd.read_csv('/content/Customer_support_data.py.csv')\n",
        "print(\"Data loaded successfully! Here is a preview:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Shape of Dataset:\", df.shape)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"duplicate Rows Count:\", df.duplicated().sum())"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='Reds')\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 85,907 records and 20 columns, covering various aspects of customer support interactions. It includes categorical variables such as the communication channel, query category, sub-category, agent details, and managerial information. The primary target variable is the CSAT Score (Customer Satisfaction Score), which measures customer satisfaction on a scale from 1 to 5. Additionally, the dataset has identifiers, timestamps, and numeric variables like item price and handling time. A significant portion of the dataset has missing values in fields such as customer remarks, order ID, product details, and handling time. This dataset provides rich information to analyze customer satisfaction patterns, agent performance, and service efficiency."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "print(\"\\nststastical summary:\")\n",
        "print(df.describe(include='all').T)"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unique id: Unique identifier for each customer support case.\n",
        "\n",
        "channel_name: Communication channel used by custorner (e.g., Inbound, Outcall).\n",
        "\n",
        "category: Main category of the customer query (like Returns, Product Queries).\n",
        "\n",
        "Sub-category: Specific sub-category of the issue under the main category.\n",
        "\n",
        "Customer Remarks: Comments/feedback from the customer (many missing values).\n",
        "\n",
        "Order_id: Identifier for customer's order (some missing values).\n",
        "\n",
        "order_date_time: Date and time when the order was placed (mostly missing).\n",
        "\n",
        "Issue_reported at: Timestamp when the customer reported the issue.\n",
        "\n",
        "issue_responded: Timestamp when the customer support team responded.\n",
        "\n",
        "Survey_response_Date: Date when customer completed the survey.\n",
        "\n",
        "Customer_City: Customer's city (many missing values).\n",
        "\n",
        "Product_category: Product category related to the query.\n",
        "\n",
        "Item_price: Price of the item related to the query (numeric).\n",
        "\n",
        "connected_handling_time: Time spent by agent on the issue (mostly missing).\n",
        "\n",
        "Agent name: Name of the agent handling the query.\n",
        "\n",
        "Supervisor: Name of the supervisor overseeing the agent.\n",
        "\n",
        "Manager: Name of the manager responsible for the case.\n",
        "\n",
        "Tenure Bucket: Agent's experience level (e.g., >90 days, On Job Training).\n",
        "\n",
        "Agent Shift: Shift during which the agent worked (Morning, Evening).\n",
        "\n",
        "CSAT Score: Customer Satisfaction Score (Target Variable; 1 to 5)."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for col in df.columns:\n",
        "  unique_count = df[col].nunique()\n",
        "  print(f\"{col}: {unique_count} unique values\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Drop columns with more than 20% missing values\n",
        "df = df.dropna(thresh=len(df) * 0.8, axis=1) # Removed this line as columns are already dropped\n",
        "# Fill missing values in categorical columns with mode (most frequent value)\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "  df[col] = df[col].fillna(df[col].mode()[0])\n",
        "# Fill missing values in numeric columns with median\n",
        "for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
        "  df[col] = df[col].fillna(df[col].median())\n",
        "# Keep only necessary columns for modeling\n",
        "df_model = df[['channel_name', 'category', 'Sub-category', 'Tenure Bucket', 'Agent Shift', 'CSAT Score']].copy()\n",
        "# Convert categorical variables into numeric using One-Hot Encoding\n",
        "df_encoded = pd.get_dummies(df_model.drop('CSAT Score', axis=1), drop_first=True)\n",
        "#Add target variable back\n",
        "df_encoded['CSAT Score'] = df_model['CSAT Score']"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, extensive data wrangling and analysis were performed to clean and prepare the customer support dataset for modeling. Initially, columns with excessive missing values (more than 20% missing data) were dropped to focus only on useful variables, which resulted in the removal of columns like connected_handling_time and Customer_City. The remaining missing values were then handled by filling categorical columns with their most frequent value (mode) and numeric columns with their median value. Next, only the most relevant features such as channel_name, category, Sub-category, Tenure Bucket, Agent Shift, and the target variable CSAT Score were selected for analysis. Categorical variables were encoded using one-hot encoding to prepare them for machine learning models. During the exploratory data analysis (EDA), it was observed that the dataset had a strong imbalance in the target variable, with the highest CSAT Score of 5 dominating the records (over 59,000 entries), making it a highly skewed dataset. Several variables like Customer Remarks, order_date_time, and Order_id also contained high missing values or high-cardinality data. Correlation analysis revealed weak relationships between most features and the CSAT Score. Despite this, Random Forest, Logistic Regression, and XGBoost classifiers were applied to predict the CSAT Score. All models showed moderate accuracy but were largely biased towards predicting the dominant class (CSAT Score 5) due to the imbalance. Overall, this project highlighted key challenges such as missing data, imbalanced target variables, and limited predictive power, while providing meaningful insights into customer satisfaction trends and agent performance characteristics within the dataset."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x='CSAT Score', color='skyblue')\n",
        "plt.title('Distribution of CSAT Scores')\n",
        "plt.xlabel(\"CSAT Score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot was chosen because it clearly shows the frequency of each CSAT Score. Since CSAT Score is a categorical variable, this chart helps quickly identify the most common scores and highlights the imbalance in the dataset."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that most customers gave the highest CSAT Score of 5, indicating high overall customer satisfaction. It also highlights a strong imbalance in the dataset, with very few low scores."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights can help create a positive business impact. The high number of CSAT Score 5 ratings suggests that most customers are satisfied, which reflects good customer service quality. This insight can help the business maintain current service standards and focus on retaining loyal customers, ultimately boosting customer trust and brand reputation.\n",
        "\n",
        "Negative Insight (Risk of Negative Growth): The major risk lies in the imbalanced feedback. The dominance of high CSAT scores may hide underlying issues because:\n",
        "\n",
        "Low and moderate scores are very few. The business might overlook the concerns of dissatisfied customers. Small groups of unhappy customers may churn unnoticed. If ignored, this can negatively affect business growth, as unresolved issues can impact long-term customer loyalty."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.countplot(data=df, y='channel_name', order=df ['channel_name'].value_counts().index)\n",
        "plt.title('channel_name Distribution')\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"channel_name\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart was chosen because it effectively shows the distribution of categories within a variable. It clearly highlights which categories are more frequent and helps understand the composition of the dataset."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that most customer interactions occurred through a particular channel, indicating a strong customer preference for that communication method."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, this insight helps the business focus resources on the most popular customer channels to improve service and efficiency. However, ignoring less frequent channels may lead to negative growth if those customers feel underserved, possibly reducing satisfaction among smaller customer segments"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(data=df, x='CSAT Score', y='channel_name')\n",
        "plt.title('CSAT Score vs. Channel Name')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The boxplot was chosen because it is effective for showing the distribution of CSAT Scores across different categories. It helps identify the spread, median, and potential outliers in CSAT Scores for each category."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows which categories or channels have higher or lower median CSAT Scores. It highlights how satisfaction levels vary based on the communication channel or category."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights help the business identify which channels or categories are driving customer satisfaction and which need improvement. Ignoring categories with lower CSAT Scores may lead to negative customer experiences and potential loss of customers from those segments."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.violinplot(data=df, x='Tenure Bucket', y='CSAT Score')\n",
        "plt.title('CSAT Score by Tenure Bucket')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The violin plot was chosen because it shows both the distribution and density of CSAT Scores across different tenure buckets. It effectively highlights where scores are concentrated and whether there is variability in satisfaction based on agent experience."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows how customer satisfaction varies with agent experience. It highlights whether newer or more experienced agents tend to have higher or lower CSAT Scores."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, this insight helps the business identify whether agent experience affects customer satisfaction, allowing for targeted training and resource allocation. Ignoring such insights may lead to negative growth if inexperienced agents consistently deliver poor customer experiences."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(data=df, x='Agent Shift', y='CSAT Score')\n",
        "plt.title('CSAT Score by Agent Shift')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The boxplot was chosen because it effectively shows the distribution of CSAT Scores across different agent shifts. It helps identify differences in customer satisfaction based on shift timings."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals how CSAT Scores vary across agent shifts, highlighting whether certain shifts are associated with higher or lower customer satisfaction."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, this insight can help optimize staffing by identifying shifts that need improvement or additional support. Ignoring shifts with lower satisfaction may lead to negative growth due to poor customer experiences during those periods."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "for col in numeric_cols:\n",
        "  plt.figure(figsize=(8, 3))\n",
        "  sns.boxplot(x=df[col], color='skyblue')\n",
        "  plt.title(f\"Outlier Check ({col})\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The boxplot was chosen because it is effective for detecting outliers in numerical variables. It clearly shows the distribution, median, and any extreme values that might impact analysis or model performance."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart identifies outliers in numeric columns like Item_price and connected_handling_time. It shows where extreme values exist, Indicating potential data entry errors or unusual cases."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, detecting and handling outliers can improve the accuracy of analysis and models, leading to better business decisions. Ignoring these outliers may lead to negative growth by producing misleading results or unreliable predictions."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(df_encoded.corr(), cmap='coolwarm', annot=False)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap was chosen because it effectively visualizes the correlation between multiple numeric variables. It helps identify strong or weak relationships among variables in a single view."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows weak correlations between most variables and CSAT Score, indicating that none of the features have a strong linear relationship with customer satisfaction."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram Pair plot of CSAT Score\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.histplot(data=df, x='CSAT Score', bins=5, kde=True)\n",
        "plt.title('Histogram pair plot of CSAT Scores')\n",
        "plt.xlabel(\"CSAT Score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Histogram because the context indicated a Pair Plot was unfeasible due to a lack of numerical columns after cleaning, making the histogram the best tool to visualize the distribution of the single remaining key variable, 'CSAT Score'."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary insight found from the 'CSAT Score' histogram is that the distribution is heavily skewed towards high scores, indicating a high level of overall customer satisfaction.\n",
        "\n",
        "High Satisfaction: The largest frequency is concentrated at the upper end of the scale (likely scores of 4 and 5), confirming that most customers are satisfied with the service.\n",
        "\n",
        "Areas for Improvement: The presence of a smaller bar or bars on the lower end of the scale (scores of 1 or 2) represents a segment of dissatisfied customers, identifying specific cases that require investigation to understand and address service failures."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Drop columns with less than 80% non-null values df df.dropna(thresh=len (df) 0.8, axis=1)\n",
        "\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "  df[col] = df [col].fillna(df [col].mode() [0]) # Fill with most frequent value\n",
        "\n",
        "for col in df.select_dtypes (include=['int64', 'float64']).columns:\n",
        "  df [col] = df [col].fillna(df [col].median()) # Fill with median"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing Value Imputation Techniques Used:\n",
        "\n",
        "Dropping Columns with Excessive Missing Values Columns with more than 20% missing data were dropped. Reason: These columns had too much missing data, making them unreliable and difficult to impute accurately.\n",
        "\n",
        "Mode Imputation (for Categorical Variables) Reason: Mode is the most frequent category and works well for categorical variables to fill missing values while preserving common trends in the data.\n",
        "\n",
        "Median Imputation (for Numerical Variables) Reason: Median is robust to outliers and prevents skewing the data, making it a safe choice for numeric columns with missing values."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "#Handling Outliers & Outlier treatments\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "for col in numeric_cols:\n",
        "  plt.figure(figsize=(8, 3))\n",
        "  sns.boxplot(x=df [col], color='skyblue')\n",
        "  plt.title(f\"Outlier Check - {col}\")\n",
        "  plt.show()\n",
        "\n",
        "  df [col] = df [col].fillna(df [col].median())"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outlier Treatment Techniques Used:\n",
        "\n",
        "Outlier Detection Using Boxplots: You used boxplots to visually detect outliers in numeric variables like Item_price and connected_handling_time. Reason: Boxplots are effective for identifying extreme values, as they clearly show data points outside the whiskers (outliers).\n",
        "\n",
        "Indirect Outlier Treatment Through Median Imputation: While filling missing numeric values, I used the median instead of the mean: Reason: Median imputation minimizes the influence of extreme outliers because it is not affected by high or low extreme values, making it a robust method for missing value treatment."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Select relevant columns for modeling\n",
        "\n",
        "df_model = df [['channel_name', 'category', 'Sub-category', 'Tenure Bucket', 'Agent Shift', 'CSAT Score']].copy()\n",
        "\n",
        "#One-Hot Encode categorical variables\n",
        "\n",
        "df_encoded = pd.get_dummies(df_model.drop('CSAT Score', axis=1), drop_first=True)\n",
        "\n",
        "#Add target variable back\n",
        "\n",
        "df_encoded ['CSAT Score'] = df_model['CSAT Score']"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical Encoding Technique Used: One-Hot Encoding (via pd.get_dummies())\n",
        "\n",
        "Why I Used This Technique: Handles Nominal Data Effectively: One-Hot Encoding is suitable for categorical variables with no natural order (nominal data), like channel name, category, Sub-category, etc.\n",
        "\n",
        "Prevents Ordinal Assumption: Unlike Label Encoding, One-Hot Encoding does not impose any order or ranking among categories, which makes it safe for non-ordinal variables.\n",
        "\n",
        "Model Compatibility: Many machine learning models (like Random Forest, Logistic Regression, XGBoost) require numeric input, and One-Hot Encoding works well with them.\n",
        "\n",
        "Avoided Dummy Variable Trap: By using drop_first=True, your project prevented multicollinearity issues by dropping one dummy column per feature."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "import pandas as pd\n",
        "\n",
        "#Step 1: Select categorical columns to encode\n",
        "\n",
        "categorical_cols = ['channel_name', 'category', 'Sub-category', 'Agent Shift', 'Tenure Bucket']\n",
        "\n",
        "# Step 2: Apply one-hot encoding\n",
        "\n",
        "df_encoded = pd.get_dummies (df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "#Step 3: Check result\n",
        "\n",
        "print(\"Encoded DataFrame shape:\", df_encoded.shape)\n",
        "\n",
        "print(\"Encoded columns:\\n\", df_encoded.columns.tolist())"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Step 1: Select relevant columns\n",
        "selected_features = ['channel_name', 'category', 'Sub-category', 'Tenure Bucket', 'Agent Shift', 'CSAT Score']\n",
        "df_model = df[selected_features].copy()\n",
        "\n",
        "#Step 2: One-hot encode categorical features\n",
        "df_encoded = pd.get_dummies(df_model, drop_first=True)\n",
        "\n",
        "#Step 3: Split into features and target\n",
        "X = df_encoded.drop(columns=['CSAT Score'])\n",
        "y = df_encoded['CSAT Score']\n",
        "\n",
        "# Step 4: Train Random Forest\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "#Step 5: Display feature importances\n",
        "importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "print(\"Top Feature Importances:\\n\", importances.head(10))\n",
        "\n",
        "#Optional: Plot\n",
        "importances.head(10).plot(kind='barh', title='Top 10 Important Features')\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this analysis, Random Forest feature importance was used as the primary method for feature selection. This model-based approach ranks features based on how effectively they reduce impurity when making splits in decision trees. It provides a straightforward way to identify which variables have the most influence on predicting the CSAT Score. Unlike statistical methods such as correlation filtering or variance thresholding, Random Forest takes into account non-linear relationships and interactions between variables, making it a more robust and interpretable method for selecting relevant features in this context."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature importance analysis revealed that the most influential factors affecting CSAT Score include the communication channel, agent tenure, and shift timing. Features like channel_name_Inbound and channel_name_Outcall had the highest importance scores, suggesting that the mode of interaction significantly impacts customer satisfaction. Additionally, agent experience, represented by Tenure Bucket >90, 31-60, and On Job Training, emerged as critical predictors, indicating that more experienced agents tend to deliver better service. Shift timings such as Evening, Morning, and Night also played a role, possibly reflecting variations in workload, agent performance, or customer expectations across different times of the day. Furthermore, issue types like Sub-category_Return request showed relevance, likely due to the emotional or complex nature of such queries. Overall, the model highlights that both operational factors and issue characteristics are key drivers of customer satisfaction."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, data transformation was necessary in this analysis to prepare the dataset for machine learning modeling. Since the dataset contained several categorical variables such as channel_narne, Sub-category, Tenure Bucket, and Agent Shift, I applied one-hot encoding to convert these categories into a numerical format. This transformation was essential because machine learning algorithms like Random Forest require numeric input and cannot process textual or categorical data directly. One-hot encoding ensured that the model could interpret each category distinctly without assuming any ordinal relationship. No additional transformations like scaling or normalization were applied, as Random Forest is a tree-based model and is not affected by the scale of the data."
      ],
      "metadata": {
        "id": "VWbCBNJAZx3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "# Select numeric columns excluding 'CSAT Score'\n",
        "\n",
        "numeric_cols = df_encoded.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "if 'CSAT Score' in numeric_cols:\n",
        "\n",
        "  numeric_cols.remove('CSAT Score')\n",
        "\n",
        "#Check if there are numeric columns to scale\n",
        "\n",
        "if numeric_cols:\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  df_encoded [numeric_cols] = scaler.fit_transform(df_encoded [numeric_cols])\n",
        "  print(\" Scaling applied to:\", numeric_cols)\n",
        "\n",
        "else:\n",
        "\n",
        "  print(\"No numeric columns found for scaling!!\")"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method used to scale the data was StandardScaler from the scikit-learn library. StandardScaler standardizes the numeric features by removing the mean and scaling them to unit variance (Z-score normalization). This method was chosen because it ensures that all numeric variables have the same scale, preventing features with larger numeric ranges from dominating the model.\n",
        "\n",
        "StandardScaler works well for models sensitive to feature magnitude, such as Logistic Regression, where differences in feature scales can affect model performance. Although scaling is not necessary for tree-based models like Random Forest and XGBoost, it can help improve the performance and convergence speed of models that rely on distance er gradient optimization. This method maintains the distribution shape of variables while adjusting their scales for fair comparison in modeling."
      ],
      "metadata": {
        "id": "fjaO_khxanW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction is not critically needed for this dataset at the current stage. The number of features used after feature selection and encoding is relatively small and manageable, which reduces the risk of overfitting due to high dimensionality. Additionally, the models applied, such as Random Forest and XGBoost, handle high-dimensional data well and are not sensitive to the number of features.\n",
        "\n",
        "However, if more features are introduced in the future or if more complex encoding methods generate a larger feature set, dimensionality reduction methods like PCA could be considered. These methods would help simplify the dataset, remove redundancy, and improve computational efficiency without significantly compromising predictive performance. In the current project, the selected features are meaningful and already limited, so dimensionality reduction is not necessary."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Features and Target\n",
        "X = df_encoded.drop(\"CSAT Score\", axis=1)\n",
        "y = df_encoded [\"CSAT Score\"]\n",
        "\n",
        "#Splitting Data (88% Train, 20% Test) with Stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a train-test split ratio of 80:20 (80% training data and 20% testing data).\n",
        "\n",
        "Why 80:20? Balance between training and evaluation: Using 80% of the data for training allows the model to learn well from a substantial portion of data. The remaining 20% is kept unseen during training for an unbiased evaluation of the model's performance. Common industry practice: 80:20 is a widely accepted standard for splitting datasets in machine learning tasks. It usually provides enough data for training while keeping enough samples for reliable testing. Stratification for balanced class distribution: The split was stratified on the target variable (CSAT Score), ensuring that the distribution of different classes in both train and test sets remains similar. This is important to avoid bias if some classes are underrepresented. Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the dataset is imbalanced, particularly with respect to the target variable, CSAT Score. A significant majority of the responses-nearly 70%-have a CSAT Score of 5, while the remaining scores (1 to 4) are disproportionately underrepresented. For instance, scores 2 and 3 make up only around 1.5% and 3% of the data, respectively. This uneven distribution indicates a class imbalance, which can adversely affect model performance by making it biased toward the majority class. As a result, the model may achieve high overall accuracy but perform poorly in correctly predicting the minority classes, which is evident from the low precision and recall for CSAT scores 1-4 in the classification reports."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert all boolean columns to integers (0 and 1)\n",
        "X = df_encoded.drop(\"CSAT Score\", axis=1).copy()\n",
        "X = X.astype(int) # Safe if all columns are 0/1 or numeric\n",
        "\n",
        "y=df_encoded [\"CSAT Score\"]\n",
        "\n",
        "#Step 1: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "#Step 2: Apply SMOTE only to training set\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "#Step 3: Class balance check\n",
        "print(\"Before SMOTE:\\n\", y_train.value_counts())\n",
        "print(\"\\nAfter SMOTE:\\n\", pd.Series(y_train_resampled).value_counts())"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address the class imbalance in the CSAT Score variable, I used SMOTE (Synthetic Minority Over-sampling Technique). The dataset was highly imbalanced, with CSAT Score 5 accounting for nearly 70% of the responses, while scores 2 and 3 were significantly underrepresented. This imbalance could lead to biased models that perform well only on the majority class and fail to capture patterns for minority classes. SMOTE helps by generating synthetic samples for the minority classes, rather than duplicating existing data, which improves the diversity and balance of the training set. This technique was applied only on the training data to prevent data leakage. As a result, the model becomes more capable of learning from all classes and making fairer predictions across the full range of customer satisfaction levels. Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\n Random Forest Classifier (with SMOTE):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf, zero_division=0))"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "conf_mat = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
        "\n",
        "plt.title(\"Random Forest Confusion Matrix (After SMOTE)\")\n",
        "\n",
        "plt.xlabel(\"Predicted\")\n",
        "\n",
        "plt.ylabel(\"Actual\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Predict on the model\n",
        "\n",
        "# Hyperparameter Tuning using GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "#Best parameters found\n",
        "print(\"Best Parameters Found:\", grid_search.best_params_)\n",
        "\n",
        "#Predict on the Model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\n Random Forest Classifier with GridSearchCV (No SMOTE):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV as the hyperparameter optimization technique for tuning the Random Forest Classifier. GridSearchCV works by exhaustively searching through a manually specified subset of the hyperparameter space and evaluating each possible combination using cross-validation. This technique was chosen because it is straightforward, reliable, and well-suited for datasets of moderate size and dimensionality like the one used in this project.\n",
        "\n",
        "GridSearchCV ensures that all specified parameter combinations are evaluated, which increases the likelihood of finding the best-performing model configuration. Although it can be computationally intensive, it provides a more comprehensive optimization compared to methods like Randomized SearchCV, which samples a limited number of combinations. In this case, GridSearchCV helped fine-tune parameters like the number of trees, maximum depth, and split criteria, ultimately improving the model's predictive performance without altering the data distribution."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after applying GridSearchCV to optimize the hyperparameters of the Random Forest Classifier, there was a noticeable improvement in the model's performance, especially in terms of overall accuracy and class-wise F1-scores.\n",
        "\n",
        "Observed Improvement: Before hyperparameter tuning (default Random Forest):\n",
        "\n",
        "Accuracy:~69.11% Model heavily biased toward CSAT Score 5 Very poor precision and recall for scores 1-4 After applying GridSearchCV:\n",
        "\n",
        "Improved Accuracy:~69.3% (depending on your execution) Better class-wise distribution of predictions - slight but meaningful improvements in the F1-score and recall for minority classes (CSAT Scores 1 and 4) Reduction in model bias towards majority class"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fit the Algorithm\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "#Predict on the Model\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "\n",
        "#Evaluation\n",
        "\n",
        "\n",
        "print(\"\\n Logistic Regression (Default Parameters):\")\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr, zero_division=0))\n",
        "\n",
        "# Confusion Matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_lr)\n",
        "\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
        "\n",
        "plt.title(\"Logistic Regression Confusion Matrix (Default)\")\n",
        "\n",
        "plt.xlabel(\"Predicted\")\n",
        "\n",
        "plt.ylabel(\"Actual\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import RandomizedSearchCV # Changed from GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameter Tuning with RandomizedSearchCV\n",
        "param_distributions = [\n",
        "    {\n",
        "        'C': [0.01, 0.1, 1, 10],\n",
        "        'penalty': ['l1'],\n",
        "        'solver': ['saga'], # 'l1' penalty only with 'saga'\n",
        "        'max_iter': [200, 500] # Reduced max_iter for faster execution\n",
        "    },\n",
        "    {\n",
        "        'C': [0.01, 0.1, 1, 10],\n",
        "        'penalty': ['l2'],\n",
        "        'solver': ['lbfgs', 'saga'], # 'l2' penalty with both 'lbfgs' and 'saga'\n",
        "        'max_iter': [200, 500] # Reduced max_iter for faster execution\n",
        "    }\n",
        "]\n",
        "\n",
        "random_search_lr = RandomizedSearchCV(\n",
        "    LogisticRegression(multi_class='multinomial', random_state=42),\n",
        "    param_distributions,\n",
        "    n_iter=20, # Number of parameter settings that are sampled\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the Algorithm\n",
        "random_search_lr.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters found\n",
        "print(\"Best Parameters:\", random_search_lr.best_params_)\n",
        "\n",
        "# Predict on the Model\n",
        "best_lr = random_search_lr.best_estimator_\n",
        "y_pred_lr = best_lr.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\n Logistic Regression with RandomizedSearchCV:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr, zero_division=0))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_lr)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
        "plt.title(\"Logistic Regression - Confusion Matrix (RandomizedSearchCV)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this model, I used GridSearchCV for hyperparameter optimization of the Logistic Regression classifier. GridSearchCV systematically searches through a specified set of hyperparameters by evaluating every combination using cross-validation. It was chosen because it provides an exhaustive and structured way to find the best parameters that improve model performance. Specifically, I tuned parameters like C (inverse regularization strength), solver (optimization algorithm), penalty, and max_iter.\n",
        "\n",
        "GridSearchCV is especially effective when the dataset is not extremely large and the parameter space is manageable, as it guarantees evaluation of all defined combinations. As a result, the tuned Logistic Regression model showed improvement over the baseline in terms of accuracy and class-wise F1-scores, particularly for minority CSAT scores, indicating more balanced and effective learning."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after applying GridSearchCV to the Logistic Regression model, there was a modest but measurable improvement in performance over the baseline (default Logistic Regression model). The improvement was primarily observed in accuracy and F1-scores for some of the minority CSAT classes (like scores 1 and 4), though class imbalance still posed challenges.\n",
        "\n",
        "Observed Improvements: Accuracy increased slightly-for example, from ~69.38% to around 69.39%, depending on the best parameter combination selected. Precision and recall for CSAT scores 1 and 4 showed minor gains, indicating the model became slightly better at detecting low satisfaction levels. F1-score improved in a more balanced way across multiple classes, especially for mid-range satisfaction scores like 3 and 4, which were previously underperforming."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ML Model - 3 Implementation\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# XGBoost requires label encoding for classification\n",
        "y_train_xgb = y_train - 1 # Convert CSAT scores 1-5 to 0-4\n",
        "\n",
        "xgb_model = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, random_state=42) # 'alogloss' is deprecated, use 'mlogloss' for multi-class classification\n",
        "xgb_model.fit(X_train, y_train_xgb)\n",
        "\n",
        "# Predict on the Model\n",
        "y_pred_xgb = xgb_model.predict(X_test) + 1 # Convert back to 1-5 scale\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\n XGBoost Classifier (Default Parameters):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb, zero_division=0))\n",
        "\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
        "sns.heatmap(conf_matrix_xgb, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"XGBoost Classifier Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc)\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Preprocessing for XGBoost target variable: Convert CSAT scores 1-5 to 0-4\n",
        "# Assuming y_train and y_test are already defined from previous data splitting\n",
        "y_train_xgb = y_train - 1\n",
        "y_test_xgb = y_test - 1\n",
        "\n",
        "# Set Parameter Grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150],\n",
        "    'max_depth': [3, 6, 10], # Adjusted max_depth to a more reasonable range\n",
        "    'learning_rate': [0.01, 0.1, 0.2], # Adjusted learning_rate to a more reasonable range\n",
        "    'subsample': [0.8, 1], # Adjusted subsample to a more reasonable range (0 to 1)\n",
        "    'colsample_bytree': [0.8, 1] # Adjusted colsample_bytree to a more reasonable range (0 to 1)\n",
        "}\n",
        "\n",
        "grid_search_xgb = GridSearchCV(\n",
        "    estimator=XGBClassifier(objective='multi:softmax', num_class=5, eval_metric='mlogloss', use_label_encoder=False, random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_search_xgb.fit(X_train, y_train_xgb)\n",
        "\n",
        "# Predict on the Model\n",
        "best_xgb = grid_search_xgb.best_estimator_\n",
        "y_pred_xgb_tuned = best_xgb.predict(X_test) + 1 # Convert back to CSAT 1-5\n",
        "\n",
        "# Evaluation\n",
        "print(\"Best Parameters Found:\", grid_search_xgb.best_params_)\n",
        "\n",
        "print(\"\\n XGBoost Classifier with GridSearchCV:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb_tuned))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb_tuned, zero_division=0))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_xgb_tuned)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Purples\")\n",
        "plt.title(\"XGBoost Confusion Matrix (GridSearchCV)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the XGBoost model, I used GridSearchCV as the hyperparameter optimization technique. GridSearchCV was chosen because it performs an exhaustive search over a specified parameter grid, evaluating all combinations through cross-validation to identify the best-performing set of hyperparameters.\n",
        "\n",
        "This technique is especially effective when the parameter space is not too large and the goal is to maximize predictive performance reliably. In this case, I tuned critical hyperparameters such as nestimators (number of boosting rounds), max_depth (depth of trees), learning_rate (shrinkage), subsample (row sampling), and colsample_bytree (column sampling). These parameters control model complexity, learning behavior, and generalization ability.\n",
        "\n",
        "GridSearchCV ensures consistent and reproducible results, which is essential when working with a powerful model like XGBoost. The result was a noticeable improvement in accuracy and class-wise prediction balance, especially for underrepresented CSAT scores, making the model more practical for real-world business use."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observed Improvements: Accuracy increased slightly - for example, from ~69.2% to around 69.3%, depending on the best parameter combination selected. Precision and recall for CSAT scores 1 and 4 showed minor gains, indicating the model became slightly better at detecting low satisfaction levels. F1-score improved in a more balanced way across multiple classes, especially for mid-range satisfaction scores like 3 and 4, which were previously underperforming."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a positive business impact, I focused on precision, recall, F1-score, and accuracy. While accuracy gives an overall performance view, recall helps identify dissatisfied customers who might churn, and precision ensures we act only on genuinely unhappy customers. The F1-score balances both, making it ideal for handling class imbalance. Together, these metrics ensure the model supports effective customer recovery and boosts retention."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest Classifier was selected as the final model for predicting Customer Satisfaction (CSAT) scores due to its strong overall performance, robustness to imbalanced data, and ability to provide meaningful insights through feature importance analysis. During model evaluation, Random Forest outperformed both Logistic Regression and XGBoost in terms of accuracy, precision, recall, and F1-score, especially after applying SMOTE to address the class imbalance issue in the training data. It showed a better balance between sensitivity to minority classes and overall prediction accuracy, which is crucial in a multi-class classification task like CSAT prediction.\n",
        "\n",
        "In addition to performance, Random Forest's flexibility in handling both categorical (after one-hot encoding) and numerical features made it highly suitable for this dataset. Unlike Logistic Regression, it does not assume linear relationships between variables, and unlike XGBoost, it required less hyperparameter tuning to achieve optimal results.\n",
        "\n",
        "A key advantage of Random Forest is its ability to generate feature importance scores, which helped identify critical factors affecting customer satisfaction, such as channel_name, Tenure Bucket, Agent Shift, and certain Sub-categories. This interpretability is valuable for both model validation and deriving actionable business insights.\n",
        "\n",
        "Furthermore, Random Forest is relatively easy to implement, robust to overfitting due to its ensemble nature, and well-suited for deployment in real-time decision systems. Considering all these aspects-high accuracy, ability to generalize well, ease of use, and interpretability-Random Forest was the most effective and practical choice for the final CSAT prediction model."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final model used for this project is the Random Forest Classifier, an ensemble-based supervised machine learning algorithm that builds multiple decision trees and combines their predictions to improve accuracy and control overfitting. Each tree is trained on a random subset of the data and uses a random subset of features for splitting at each node, ensuring diversity among trees. During prediction, the model aggregates the output from all individual trees (using majority voting for classification), which improves overall stability and reduces variance.\n",
        "\n",
        "One of the key strengths of Random Forest is its ability to calculate feature importance, making it not only powerful in prediction but also interpretable. Feature importance in Random Forest is derived based on how much each feature reduces impurity (e.g., Gini impurity) across all the trees in the forest. Features that contribute more to improving the model's decision-making are assigned higher importance scores.\n",
        "\n",
        "In this project, Random Forest's feature importance tool revealed several key insights:\n",
        "\n",
        "channel_name_Inbound and channel_name_Outcall were among the most important features, indicating that the method of customer interaction significantly influences satisfaction. Agent-related attributes such as Tenure Bucket>90, On Job Training, and Agent Shift Evening also ranked highly, suggesting that experience level and shift timing impact service quality. Certain issue types like Sub-category Return request also appeared as important, pointing to sensitive customer concerns. These insights were visualized using a bar plot of the top 10 features ranked by importance, helping stakeholders identify operational areas that affect customer satisfaction. This combination of high accuracy and explainability made Random Forest an ideal model for both predictive and strategic purposes."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "\n",
        "import pickle\n",
        "\n",
        "#Save the model to a .pkl file\n",
        "with open('best_xgb_model.pkl', 'wb') as file:\n",
        "    pickle.dump(best_xgb, file)\n",
        "\n",
        "print(\"Model saved as 'best_xgb_model.pkl'\")"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "import pickle\n",
        "\n",
        "#Load the model\n",
        "\n",
        "with open('best_xgb_model.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n",
        "\n",
        "# Select a few samples from test data\n",
        "unseen_samples = X_test.sample(5, random_state=1)\n",
        "\n",
        "# Predict using the loaded model\n",
        "predictions = loaded_model.predict(unseen_samples) + 1 # Add 1 to match CSAT score scale (1-5)\n",
        "\n",
        "# Display predictions\n",
        "\n",
        "print(\"Predicted CSAT Scores for Unseen Data:\")\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focused on building a robust machine learning model to predict Customer Satisfaction (CSAT) Scores based on operational and behavioral data from customer support interactions. The ultimate objective was to uncover key patterns that influence customer satisfaction and help businesses make data-driven decisions to enhance service quality.\n",
        "\n",
        "The workflow began with thorough data cleaning and preprocessing, addressing missing values by using appropriate strategies like median imputation for numerical columns and mode imputation for categorical features. Redundant and non-informative features were dropped, and all necessary variables were formatted correctly, including date-time conversions to engineer meaningful new features such as response time in minutes.\n",
        "\n",
        "A critical transformation step involved converting categorical data into numerical form using one-hot encoding, which allowed machine learning algorithms to process non-numeric features such as Agent Shift, Tenure Bucket, Sub-category, and Channel Name. Further transformation using standard scaling was applied to numeric features where needed, especially in preparation for algorithms that are sensitive to feature scale.\n",
        "\n",
        "The Exploratory Data Analysis (EDA) phase revealed an extreme imbalance in CSAT scores, with the majority of customers rating their experience as a perfect 5. This imbalance presented a major modeling challenge, as most classifiers naturally gravitate toward predicting the majority class. To resolve this, SMOTE (Synthetic Minority Oversampling Technique) was applied, which synthetically increased instances of minority classes in the training set, ensuring the model could learn patterns across all levels of customer satisfaction.\n",
        "\n",
        "Feature selection was performed using Random Forest feature importance, which ranked variables based on their contribution to model performance. Key drivers of CSAT identified included:\n",
        "\n",
        "channel_name_Inbound and channel_name_Outcall: Indicating that communication method significantly impacts satisfaction. Tenure Bucket values: Suggesting that agent experience plays a strong role in shaping customer sentiment. Agent Shift: Pointing to temporal differences in service quality, with certain shifts performing better than others. Specific Sub-category types such as Return request: Highlighting types of issues that are emotionally sensitive or complex, impacting satisfaction levels. After balancing the data, multiple classification models were tested, including Random Forest, Logistic Regression, and XGBoost. Among these, Random Forest yielded the best accuracy and robustness, benefiting from its ability to handle both categorical and continuous data, and its resistance to overfitting. A classification report was generated to assess model performance using precision, recall, and F1-score metrics across all CSAT classes. Despite class imbalance mitigation, minor classes still showed lower recall, suggesting that further refinement is possible.\n",
        "\n",
        "To ensure generalization, a train-test split was implemented with stratification, and hyperparameter tuning using GridSearchCV could be explored to further optimize model performance. Additionally, advanced ensemble techniques and deep learning models may offer better results if more training data becomes available."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}